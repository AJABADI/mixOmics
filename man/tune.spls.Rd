% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune.spls.R
\name{tune.spls}
\alias{tune.spls}
\title{Tuning functions for sPLS and PLS functions}
\usage{
tune.spls(
  X,
  Y,
  test.keepX = NULL,
  test.keepY = NULL,
  ncomp,
  validation = c("Mfold", "loo"),
  nrepeat = 1,
  folds,
  mode = c("regression", "canonical", "classic"),
  measure.tune = c("cor", "RSS"),
  BPPARAM = SerialParam(),
  progressBar = FALSE
)
}
\arguments{
\item{X}{numeric matrix of predictors. \code{NA}s are allowed.}

\item{Y}{\code{if(method = 'spls')} numeric vector or matrix of continuous
responses (for multi-response models) \code{NA}s are allowed.}

\item{test.keepX}{numeric vector for the different number of variables to
test from the \eqn{X} data set}

\item{ncomp}{the number of components to include in the model.}

\item{validation}{character.  What kind of (internal) validation to use,
matching one of \code{"Mfold"} or \code{"loo"} (see below). Default is
\code{"Mfold"}.}

\item{nrepeat}{Number of times the Cross-Validation process is repeated.}

\item{folds}{the folds in the Mfold cross-validation. See Details.}

\item{progressBar}{by default set to \code{TRUE} to output the progress bar
of the computation.}

\item{already.tested.X}{Optional, if \code{ncomp > 1} A numeric vector
indicating the number of variables to select from the \eqn{X} data set on
the firsts components.}

\item{measure}{One of \code{MSE} (Mean Squared Error), 
\code{MAE} (Mean Absolute Error: MSE without the square), 
\code{Bias} (average of the differences), 
\code{MAPE} (average of the absolute errors,
 as a percentage of the actual values) or \code{R2}. 
 Default to \code{MSE}. See details.}

\item{scale}{Logical. If scale = TRUE, each block is standardized to zero
means and unit variances (default: TRUE)}

\item{tol}{Convergence stopping value.}

\item{max.iter}{integer, the maximum number of iterations.}

\item{near.zero.var}{Logical, see the internal \code{\link{nearZeroVar}}
function (should be set to TRUE in particular for data with many zero
values). Default value is FALSE}

\item{multilevel}{Design matrix for multilevel analysis (for repeated
measurements) that indicates the repeated measures on each individual, i.e.
the individuals ID. See Details.}

\item{light.output}{if set to FALSE, the prediction/classification of each
sample for each of \code{test.keepX} and each comp is returned.}

\item{cpus}{Number of cpus to use. If greater than 1, the code is run in
parallel.}
}
\value{
A list that contains: \item{error.rate}{returns the prediction error
for each \code{test.keepX} on each component, averaged across all repeats
and subsampling folds. Standard deviation is also output. All error rates
are also available as a list.} \item{choice.keepX}{returns the number of
variables selected (optimal keepX) on each component.}
\item{choice.ncomp}{returns the optimal number of components for the model
fitted with \code{$choice.keepX} and \code{$choice.keepY} }
\item{measure}{reminds which criterion was used} \item{predict}{Prediction
values for each sample, each \code{test.keepX,test.keepY}, each comp and
each repeat. Only if light.output=FALSE}
}
\description{
This function uses repeated cross-validation to tune hyperparameters such as
the number of features to select and possibly the number of components to
extract.
}
\section{folds}{
 
During a cross-validation (CV), data are randomly split into \code{M}
subgroups (folds). \code{M-1} subgroups are then used to train submodels
which would be used to predict prediction accuracy statistics for the
held-out (test) data. All subgroups are used as the test data exactly once.
If \code{validation = "loo"}, leave-one-out CV is used where each group
consists of exactly one sample and hence \code{M == N} where N is the number
of samples.
}

\section{nrepeat}{
 
The cross-validation process is repeated \code{nrepeat} times and the
accuracy measures are averaged across repeats. If \code{validation = "loo"},
the process does not need to be repeated as there is only one way to split N
samples into N groups and hence nrepeat is forced to be 1.
}

\section{measure-pls}{
 
Two measures of accuracy are available: Correlation (\code{cor}), as well as
the Residual Sum of Squares (\code{RSS}). For \code{cor}, the parameters
which would maximise the correlation between the predicted and the actual
components are chosen. The \code{RSS} measure tries to predict the held-out
data by matrix reconstruction and seeks to minimise the error between actual
and predicted values. For \code{mode='canonical'}, The X matrix is used to
calculate the \code{RSS}, while for others modes the \code{Y} matrix is used.
This measure gives more weight to any large errors and is thus sensitive to
outliers. It also intrinsically selects less number of features on the
\code{Y} block compared to \code{measure='cor'}.
}

\section{t-test-process}{
 
The optimisation process is data-driven and similar to the process detailed
in (Rohart et al., 2016), where one-sided t-tests assess whether there is a
gain in performance when incrementing the number of features or components in
the model. However, it will assess all the provided grid through pair-wise
comparisons as the performance criteria do not always change linearly with
respect to the added number of features or components.
}

\section{more}{

See also \code{?perf} for more details.
}

\examples{

\dontrun{
data(liver.toxicity)
X <- liver.toxicity$gene
Y <- liver.toxicity$clinic
set.seed(42)
tune = tune.spls( X, Y, ncomp = 3, 
                  test.keepX = c(5, 10, 15), 
                  test.keepY = c(3, 6, 8), measure = "cor", 
                  folds = 5, nrepeat = 3, progressBar = TRUE)
tune$choice.ncomp
tune$choice.keepX
tune$choice.keepY
# plot the results
plot(tune)
}
}
\references{
mixOmics article:

Rohart F, Gautier B, Singh A, Lê Cao K-A. mixOmics: an R package for 'omics
feature selection and multiple data integration. PLoS Comput Biol 13(11):
e1005752

PLS and PLS citeria for PLS regression: Tenenhaus, M. (1998). La regression
PLS: theorie et pratique. Paris: Editions Technic.

Chavent, Marie and Patouille, Brigitte (2003). Calcul des coefficients de
regression et du PRESS en regression PLS1. Modulad n, 30 1-11. (this is the
formula we use to calculate the Q2 in perf.pls and perf.spls)

Mevik, B.-H., Cederkvist, H. R. (2004). Mean Squared Error of Prediction
(MSEP) Estimates for Principal Component Regression (PCR) and Partial Least
Squares Regression (PLSR). Journal of Chemometrics 18(9), 422-429.

sparse PLS regression mode:

Lê Cao, K. A., Rossouw D., Robert-Granie, C. and Besse, P. (2008). A sparse
PLS for variable selection when integrating Omics data. Statistical
Applications in Genetics and Molecular Biology 7, article 35.

One-sided t-tests (suppl material):

Rohart F, Mason EA, Matigian N, Mosbergen R, Korn O, Chen T, Butcher S,
Patel J, Atkinson K, Khosrotehrani K, Fisk NM, Lê Cao K-A&, Wells CA&
(2016). A Molecular Classification of Human Mesenchymal Stromal Cells. PeerJ
4:e1845.
}
\seealso{
\code{\link{splsda}}, \code{\link{predict.splsda}} and
http://www.mixOmics.org for more details.
}
\author{
Kim-Anh Lê Cao, Benoit Gautier, Francois Bartolo, Florian Rohart,
Al J Abadi
}
\keyword{multivariate}
\keyword{regression}
